# LLaVA-Model-Zoo-Large-Language-and-Vision-Assistant
A lightweight, private, and easy-to-use multimodal AI setup combining LLaVA (vision + language) with the Ollama runtime so you can do visual question answering, image understanding, and general vision-language reasoning entirely on your machineâ€”no cloud required.
